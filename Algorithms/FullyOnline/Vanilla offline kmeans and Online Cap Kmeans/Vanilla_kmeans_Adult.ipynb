{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddbe973a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= 'Adult'  #Name of the dataset from ['Adult','Bank']\n",
    "order= 'random'  # Order of data points with respect to protected group - random or arbitrary\n",
    "\n",
    "Ktarget = 10   #Target or desired number of clusters \n",
    "\n",
    "\n",
    "#Point 1. Capture all costs as squared euclidean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90de3729",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "import random\n",
    "import numba\n",
    "from numba import jit , njit\n",
    "from csv import writer\n",
    "import time\n",
    "from scipy.spatial import distance\n",
    "#!pip install scikit-learn-extra\n",
    "from sklearn.cluster import KMeans\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d13850b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset,Ktarget):\n",
    "    \n",
    "    flag= True\n",
    "    if dataset=='Adult':\n",
    "        _filename = 'adult_p.csv'\n",
    "        OriCenters=[]\n",
    "    elif dataset=='Bank':\n",
    "        _filename = 'bank_p.csv'\n",
    "        OriCenters=[]\n",
    "        \n",
    "    elif dataset=='Synthetic-1d':\n",
    "        \n",
    "        OriCenters=[1]\n",
    "        cluster1 = np.random.normal([1], 2, 1000)\n",
    "        \n",
    "            \n",
    "        df=pd.DataFrame(cluster1, columns=['X'])\n",
    "        for kk in range(1,Ktarget):\n",
    "           \n",
    "            cluster1 = np.random.normal([1+7*kk],2 , 1000)\n",
    "            OriCenters.append([1+7*kk])\n",
    "           \n",
    "          \n",
    "            df1=pd.DataFrame(cluster1, columns=['X'])\n",
    "            df=pd.concat([df,df1])\n",
    "            \n",
    "      \n",
    "        df = df.reset_index()  \n",
    "        \n",
    "        df = df.drop(df.columns[0],axis=1)\n",
    "       \n",
    "        df=shuffleDataset(df)\n",
    "        OriCenters=np.array(OriCenters)\n",
    "        flag= False\n",
    "        \n",
    "        \n",
    "    elif dataset=='Synthetic-2d-over':\n",
    "        \n",
    "\n",
    "        cluster1 = np.random.multivariate_normal([1,3], [[1, 0], [0, 1]], 1000)\n",
    "        OriCenters=[[1,3]]\n",
    "            \n",
    "        df=pd.DataFrame(cluster1, columns=['X','Y'])\n",
    "        for kk in range(1,Ktarget):\n",
    "           \n",
    "            cluster1 = np.random.multivariate_normal([1+5*kk,3], [[1, 0], [0, 1]], 1000)\n",
    "            \n",
    "            OriCenters.append([1+5*kk,3])\n",
    "          \n",
    "            df1=pd.DataFrame(cluster1, columns=['X','Y'])\n",
    "            df=pd.concat([df,df1])\n",
    "            \n",
    "      \n",
    "        df = df.reset_index()\n",
    "        df = df.drop(df.columns[0],axis=1)\n",
    "        #print(df)\n",
    "        \n",
    "        \n",
    "        plot2Ddata(df['X'],df['Y'])\n",
    "        df=shuffleDataset(df)\n",
    "        OriCenters=np.array(OriCenters)\n",
    "        flag= False\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "    elif dataset=='Synthetic-2d': #well separated\n",
    "        \n",
    "\n",
    "        cluster1 = np.random.multivariate_normal([1,3], [[1, 0], [0, 1]], 1000)\n",
    "        OriCenters=[[1,3]]\n",
    "            \n",
    "        df=pd.DataFrame(cluster1, columns=['X','Y'])\n",
    "        for kk in range(1,Ktarget):\n",
    "           \n",
    "            cluster1 = np.random.multivariate_normal([1+7*kk,3], [[1, 0], [0, 1]], 1000)\n",
    "            \n",
    "            OriCenters.append([1+7*kk,3])\n",
    "          \n",
    "            df1=pd.DataFrame(cluster1, columns=['X','Y'])\n",
    "            df=pd.concat([df,df1])\n",
    "            \n",
    "      \n",
    "        df = df.reset_index()\n",
    "        df = df.drop(df.columns[0],axis=1)\n",
    "        #print(df)\n",
    "        \n",
    "        \n",
    "        plot2Ddata(df['X'],df['Y'])\n",
    "        df=shuffleDataset(df)\n",
    "        OriCenters=np.array(OriCenters)\n",
    "        flag= False\n",
    "        \n",
    "    elif dataset=='Synthetic-1d-Over':\n",
    "        \n",
    "        OriCenters=[1]\n",
    "        cluster1 = np.random.normal([1], 2, 1000)\n",
    "        \n",
    "            \n",
    "        df=pd.DataFrame(cluster1, columns=['X'])\n",
    "        for kk in range(1,Ktarget):\n",
    "           \n",
    "            cluster1 = np.random.normal([1+5*kk],2 , 1000)\n",
    "            OriCenters.append([1+5*kk])\n",
    "           \n",
    "          \n",
    "            df1=pd.DataFrame(cluster1, columns=['X'])\n",
    "            df=pd.concat([df,df1])\n",
    "            \n",
    "      \n",
    "        df = df.reset_index()  \n",
    "        \n",
    "        df = df.drop(df.columns[0],axis=1)\n",
    "       \n",
    "        df=shuffleDataset(df)\n",
    "        OriCenters=np.array(OriCenters)\n",
    "        flag= False\n",
    "        \n",
    "    elif dataset=='Synthetic-arb':\n",
    "        flag= False\n",
    "        pass\n",
    "    \n",
    "    if flag:\n",
    "        \n",
    "        df = pd.read_csv(_filename, sep=',')#[:10000]\n",
    "        df= df.round(decimals=5)\n",
    "        df = df.dropna()\n",
    "    \n",
    "        if order=='random':\n",
    "            df=shuffleDataset(df)\n",
    "        \n",
    "    print('Dataset loaded........')\n",
    "    \n",
    "    #df.to_csv('SeededData/'+str(dataset)+'_seed_'+str(seed)+'.csv',index=False)\n",
    "    \n",
    "    return df,OriCenters\n",
    "        \n",
    "def plot2Ddata(x,y):\n",
    "    plt.plot(x, y, 'x')\n",
    "    plt.axis('equal')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92934584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffleDataset(df):\n",
    "\n",
    "    df = shuffle(df)\n",
    "    df = shuffle(df)    #random with respect to group and distance\n",
    "    df = shuffle(df)\n",
    "    df = df.reset_index()\n",
    "    df = df.drop(columns='index')\n",
    "    return df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f9735a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit(parallel=False)\n",
    "def EuclideanDistance(x,y):\n",
    "    sum_ = 0\n",
    "    \n",
    "    for i in range(0,len(x)):\n",
    "        \n",
    "        sum_ = sum_ + (x[i]-y[i])**2\n",
    "\n",
    "    return sum_#**0.5\n",
    "\n",
    "\n",
    "def EuclideanDistance_(x,y):\n",
    "    sum_ = 0\n",
    "#     print(x)\n",
    "#     print(y)\n",
    "#     print()\n",
    "    \n",
    "    for i in range(0,len(x)):\n",
    "        \n",
    "        sum_ = sum_ + (x[i]-y[i])**2\n",
    "\n",
    "    return sum_#**0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70ce3cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit(parallel=False)\n",
    "def Distances(rows,center): \n",
    "    # Numpy array - center and rows\n",
    "    #Finds the distance of rows to 1 center in fast parallel way\n",
    "    \n",
    "    #need centers in col\n",
    "    rr = len(rows)\n",
    "    cc = len(center) \n",
    "   \n",
    "    dist=np.zeros((rr, cc)) \n",
    "    \n",
    "    \n",
    "    \n",
    "    for indr in range(0,len(rows)):\n",
    "        \n",
    "        \n",
    "        \n",
    "        for indce in range(0,len(center)):\n",
    "            \n",
    "            dis = EuclideanDistance(rows[indr],center[indce])\n",
    "            dist[indr][indce]= dis\n",
    "            \n",
    "    \n",
    "    return dist\n",
    " \n",
    "#@njit(parallel=False)\n",
    "def sortedDistances1(centers,row): \n",
    "    # Numpy array - center and rows\n",
    "    #Finds the distance of rows to 1 center in fast parallel way\n",
    "    \n",
    "    distOfCenters = []\n",
    "    \n",
    "    \n",
    "    for indce in range(0,len(centers)): # each center is tuple of index, center vector \n",
    "        dis = EuclideanDistance_(row,centers[indce][1])\n",
    "        distOfCenters.append(tuple((dis,centers[indce][0])))  #add distance, original center id \n",
    "        \n",
    "            \n",
    "    #sorted_centers = sorted(distToCenters, key=lambda x: x[0])\n",
    "    distOfCenters.sort()  #inPlace sorts \n",
    "    \n",
    "    \n",
    "    return distOfCenters\n",
    "            \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8508aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded........\n",
      "K is  292\n",
      "1692.6109032835734\n",
      "3.3974182955371184\n",
      "**********\n",
      "K is  9\n",
      "14552.941091888108\n",
      "0.05939744208285645\n",
      "**********\n",
      "K is  474\n",
      "1247.4744111656407\n",
      "4.360888299065333\n",
      "**********\n",
      "K is  12\n",
      "12396.049195803338\n",
      "16.173285085819913\n",
      "**********\n",
      "K is  806\n",
      "882.3633818875021\n",
      "3.5378875940422088\n",
      "**********\n",
      "K is  20\n",
      "9125.713173903974\n",
      "69.47643537742977\n",
      "**********\n",
      "K is  1153\n",
      "693.7397366089729\n",
      "3.161974608232994\n",
      "**********\n",
      "K is  26\n",
      "7641.797075631502\n",
      "66.38503537734097\n",
      "**********\n",
      "K is  1857\n",
      "498.1261805646882\n",
      "1.9098542986427745\n",
      "**********\n",
      "K is  36\n",
      "6167.866267763496\n",
      "48.280829137396196\n",
      "**********\n",
      "K is  3143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lxc128/.local/lib/python3.6/site-packages/ipykernel_launcher.py:102: ConvergenceWarning: Number of distinct clusters (3142) found smaller than n_clusters (3143). Possibly due to duplicate points in X.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338.2780569139212\n",
      "1.7555893675834815\n",
      "**********\n",
      "K is  54\n",
      "4826.4532383122805\n",
      "26.306529786656263\n",
      "**********\n",
      "K is  4657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lxc128/.local/lib/python3.6/site-packages/ipykernel_launcher.py:102: ConvergenceWarning: Number of distinct clusters (4652) found smaller than n_clusters (4657). Possibly due to duplicate points in X.\n",
      "/home/lxc128/.local/lib/python3.6/site-packages/ipykernel_launcher.py:102: ConvergenceWarning: Number of distinct clusters (4654) found smaller than n_clusters (4657). Possibly due to duplicate points in X.\n",
      "/home/lxc128/.local/lib/python3.6/site-packages/ipykernel_launcher.py:102: ConvergenceWarning: Number of distinct clusters (4656) found smaller than n_clusters (4657). Possibly due to duplicate points in X.\n",
      "/home/lxc128/.local/lib/python3.6/site-packages/ipykernel_launcher.py:102: ConvergenceWarning: Number of distinct clusters (4656) found smaller than n_clusters (4657). Possibly due to duplicate points in X.\n",
      "/home/lxc128/.local/lib/python3.6/site-packages/ipykernel_launcher.py:102: ConvergenceWarning: Number of distinct clusters (4655) found smaller than n_clusters (4657). Possibly due to duplicate points in X.\n",
      "/home/lxc128/.local/lib/python3.6/site-packages/ipykernel_launcher.py:102: ConvergenceWarning: Number of distinct clusters (4654) found smaller than n_clusters (4657). Possibly due to duplicate points in X.\n",
      "/home/lxc128/.local/lib/python3.6/site-packages/ipykernel_launcher.py:102: ConvergenceWarning: Number of distinct clusters (4656) found smaller than n_clusters (4657). Possibly due to duplicate points in X.\n",
      "/home/lxc128/.local/lib/python3.6/site-packages/ipykernel_launcher.py:102: ConvergenceWarning: Number of distinct clusters (4655) found smaller than n_clusters (4657). Possibly due to duplicate points in X.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246.5408262825706\n",
      "1.7502776383887753\n",
      "**********\n",
      "K is  4\n",
      "23247.601579401125\n",
      "0.0927743764899\n",
      "**********\n",
      "K is  5946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lxc128/.local/lib/python3.6/site-packages/ipykernel_launcher.py:102: ConvergenceWarning: Number of distinct clusters (5945) found smaller than n_clusters (5946). Possibly due to duplicate points in X.\n",
      "/home/lxc128/.local/lib/python3.6/site-packages/ipykernel_launcher.py:102: ConvergenceWarning: Number of distinct clusters (5943) found smaller than n_clusters (5946). Possibly due to duplicate points in X.\n",
      "/home/lxc128/.local/lib/python3.6/site-packages/ipykernel_launcher.py:102: ConvergenceWarning: Number of distinct clusters (5943) found smaller than n_clusters (5946). Possibly due to duplicate points in X.\n",
      "/home/lxc128/.local/lib/python3.6/site-packages/ipykernel_launcher.py:102: ConvergenceWarning: Number of distinct clusters (5943) found smaller than n_clusters (5946). Possibly due to duplicate points in X.\n",
      "/home/lxc128/.local/lib/python3.6/site-packages/ipykernel_launcher.py:102: ConvergenceWarning: Number of distinct clusters (5945) found smaller than n_clusters (5946). Possibly due to duplicate points in X.\n",
      "/home/lxc128/.local/lib/python3.6/site-packages/ipykernel_launcher.py:102: ConvergenceWarning: Number of distinct clusters (5945) found smaller than n_clusters (5946). Possibly due to duplicate points in X.\n",
      "/home/lxc128/.local/lib/python3.6/site-packages/ipykernel_launcher.py:102: ConvergenceWarning: Number of distinct clusters (5942) found smaller than n_clusters (5946). Possibly due to duplicate points in X.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199.4844821172353\n",
      "0.942837903883045\n",
      "**********\n",
      "K is  8\n",
      "15545.477648995022\n",
      "0.007208958807869354\n",
      "**********\n",
      "K is  7136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lxc128/.local/lib/python3.6/site-packages/ipykernel_launcher.py:102: ConvergenceWarning: Number of distinct clusters (7134) found smaller than n_clusters (7136). Possibly due to duplicate points in X.\n",
      "/home/lxc128/.local/lib/python3.6/site-packages/ipykernel_launcher.py:102: ConvergenceWarning: Number of distinct clusters (7126) found smaller than n_clusters (7136). Possibly due to duplicate points in X.\n",
      "/home/lxc128/.local/lib/python3.6/site-packages/ipykernel_launcher.py:102: ConvergenceWarning: Number of distinct clusters (7132) found smaller than n_clusters (7136). Possibly due to duplicate points in X.\n",
      "/home/lxc128/.local/lib/python3.6/site-packages/ipykernel_launcher.py:102: ConvergenceWarning: Number of distinct clusters (7133) found smaller than n_clusters (7136). Possibly due to duplicate points in X.\n",
      "/home/lxc128/.local/lib/python3.6/site-packages/ipykernel_launcher.py:102: ConvergenceWarning: Number of distinct clusters (7132) found smaller than n_clusters (7136). Possibly due to duplicate points in X.\n",
      "/home/lxc128/.local/lib/python3.6/site-packages/ipykernel_launcher.py:102: ConvergenceWarning: Number of distinct clusters (7133) found smaller than n_clusters (7136). Possibly due to duplicate points in X.\n",
      "/home/lxc128/.local/lib/python3.6/site-packages/ipykernel_launcher.py:102: ConvergenceWarning: Number of distinct clusters (7133) found smaller than n_clusters (7136). Possibly due to duplicate points in X.\n",
      "/home/lxc128/.local/lib/python3.6/site-packages/ipykernel_launcher.py:102: ConvergenceWarning: Number of distinct clusters (7133) found smaller than n_clusters (7136). Possibly due to duplicate points in X.\n",
      "/home/lxc128/.local/lib/python3.6/site-packages/ipykernel_launcher.py:102: ConvergenceWarning: Number of distinct clusters (7132) found smaller than n_clusters (7136). Possibly due to duplicate points in X.\n",
      "/home/lxc128/.local/lib/python3.6/site-packages/ipykernel_launcher.py:102: ConvergenceWarning: Number of distinct clusters (7129) found smaller than n_clusters (7136). Possibly due to duplicate points in X.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167.56887394076847\n",
      "0.9223368832060382\n",
      "**********\n",
      "K is  12\n",
      "12396.049195803338\n",
      "16.173285085819913\n",
      "**********\n",
      "K is  9565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lxc128/.local/lib/python3.6/site-packages/ipykernel_launcher.py:102: ConvergenceWarning: Number of distinct clusters (9559) found smaller than n_clusters (9565). Possibly due to duplicate points in X.\n",
      "/home/lxc128/.local/lib/python3.6/site-packages/ipykernel_launcher.py:102: ConvergenceWarning: Number of distinct clusters (9556) found smaller than n_clusters (9565). Possibly due to duplicate points in X.\n",
      "/home/lxc128/.local/lib/python3.6/site-packages/ipykernel_launcher.py:102: ConvergenceWarning: Number of distinct clusters (9560) found smaller than n_clusters (9565). Possibly due to duplicate points in X.\n",
      "/home/lxc128/.local/lib/python3.6/site-packages/ipykernel_launcher.py:102: ConvergenceWarning: Number of distinct clusters (9558) found smaller than n_clusters (9565). Possibly due to duplicate points in X.\n",
      "/home/lxc128/.local/lib/python3.6/site-packages/ipykernel_launcher.py:102: ConvergenceWarning: Number of distinct clusters (9552) found smaller than n_clusters (9565). Possibly due to duplicate points in X.\n",
      "/home/lxc128/.local/lib/python3.6/site-packages/ipykernel_launcher.py:102: ConvergenceWarning: Number of distinct clusters (9557) found smaller than n_clusters (9565). Possibly due to duplicate points in X.\n",
      "/home/lxc128/.local/lib/python3.6/site-packages/ipykernel_launcher.py:102: ConvergenceWarning: Number of distinct clusters (9557) found smaller than n_clusters (9565). Possibly due to duplicate points in X.\n",
      "/home/lxc128/.local/lib/python3.6/site-packages/ipykernel_launcher.py:102: ConvergenceWarning: Number of distinct clusters (9556) found smaller than n_clusters (9565). Possibly due to duplicate points in X.\n",
      "/home/lxc128/.local/lib/python3.6/site-packages/ipykernel_launcher.py:102: ConvergenceWarning: Number of distinct clusters (9559) found smaller than n_clusters (9565). Possibly due to duplicate points in X.\n",
      "/home/lxc128/.local/lib/python3.6/site-packages/ipykernel_launcher.py:102: ConvergenceWarning: Number of distinct clusters (9558) found smaller than n_clusters (9565). Possibly due to duplicate points in X.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122.82423255059443\n",
      "0.6163642027087823\n",
      "**********\n",
      "K is  20\n",
      "9125.713173903974\n",
      "69.47643537742977\n",
      "**********\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def CapHeur(Ktarget,X,capacities):\n",
    "    \n",
    "    n_row, n_col = X.shape\n",
    "    rand_indices = np.random.choice(n_row, size = Ktarget)\n",
    "    maxiter=100\n",
    "  \n",
    "    centroids = X[rand_indices]\n",
    "\n",
    "    \n",
    "   \n",
    "    for itr in range(maxiter):\n",
    "        distances_to_centroids = Distances(X, centroids)#distance.cdist(X, centroids, 'sqeuclidean')\n",
    "        \n",
    "\n",
    "        \n",
    "        cluster_assignment=[]\n",
    "        \n",
    "        capacities_iter= np.array(capacities,dtype=np.int64)#(tuple of form (center idx,capacity))\n",
    "        #Deep copies it into this np.array()\n",
    "        \n",
    "        for idx in range(0,len(X)):\n",
    "            #print(distances_to_centroids[idx])\n",
    "            sorted_centerIDs = np.argsort(distances_to_centroids[idx])\n",
    "            \n",
    "            for idc in sorted_centerIDs:\n",
    "            \n",
    "                if capacities_iter[int(idc)][1] >0:\n",
    "                    cluster_assignment.append(idc)\n",
    "                    capacities_iter[int(idc)][1] -=1\n",
    "                    break\n",
    "                    \n",
    "                    \n",
    "        #print(\"Found {} assignements for iteration {}.\".format(len(cluster_assignment), itr))\n",
    "               \n",
    "        cluster_assignment=np.array(cluster_assignment)   \n",
    "        \n",
    "        new_centroids=[]\n",
    "        for i  in range(Ktarget):\n",
    "            \n",
    "            assign= np.array([X[cluster_assignment == i]])\n",
    "            #print('shape of assign is ',assign)\n",
    "            med = np.mean(assign,axis=1)\n",
    "            #print('shape of med is ',med)\n",
    "            new_centroids.append(med)\n",
    "            \n",
    "            \n",
    "       \n",
    "            \n",
    "        new_centroids = np.array(new_centroids)\n",
    "        new_centroids = new_centroids.reshape(Ktarget,len(X[0]))\n",
    "        \n",
    "            \n",
    "        if np.all(centroids == new_centroids):\n",
    "                break\n",
    "        \n",
    "        centroids = new_centroids\n",
    "        #print('centroids are ',centroids)\n",
    "            \n",
    "    \n",
    "    \n",
    "    cost = []\n",
    "    for i in range(Ktarget):\n",
    "        cluster_data = X[cluster_assignment == i]\n",
    "        distances = distance.cdist(cluster_data, [centroids[i]], 'euclidean')\n",
    "        cost.append( np.sum(distances ** 2))\n",
    "    \n",
    "    return np.sum(cost)\n",
    "\n",
    "\n",
    "def LowerBoundCapHeuristic(dfValues):\n",
    "    KtargetList =[156,9,228,13,345,21,386,29,537,41,669,60,794,81,944,100,1068,118,1434,160]\n",
    "    for Ktarget in KtargetList:\n",
    "        print('K is ',Ktarget)\n",
    "        w_list = []\n",
    "        seeds=[0,100,200]#,300,400,500,600,700,800,900]\n",
    "        for seed in seeds:\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "\n",
    "            capacities=[]\n",
    "\n",
    "            for i in range(Ktarget):\n",
    "                capacities.append([i,np.ceil(len(dfValues)/Ktarget)])  #(centerId,capacity)\n",
    "\n",
    "            w_list.append(CapHeur(Ktarget,dfValues,capacities))\n",
    "\n",
    "        print(np.mean(w_list))\n",
    "        print(np.std(w_list))\n",
    "        print('*'*10)\n",
    "    return True\n",
    "\n",
    "def CostVanillaKmeans1(dfValues):\n",
    "    \n",
    "    KtargetList =[292,9,474,12,806,20,1153,26,1857,36,3143,54,4657,4,5946,8,7136,12,9565,20]\n",
    "    for Ktarget in KtargetList:\n",
    "        print('K is ',Ktarget)\n",
    "        seeds=[0,100,200,300,400,500,600,700,800,900]\n",
    "        cost = []\n",
    "        for seed in seeds:\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            kmean_obj = KMeans(n_clusters=Ktarget, init='random' ,random_state=seed).fit(dfValues)\n",
    "\n",
    "            cost.append(kmean_obj.inertia_)    #Sum of squarred euc dis \n",
    "\n",
    "\n",
    "        print(np.mean(cost))\n",
    "        print(np.std(cost))\n",
    "        print('*'*10)\n",
    "        \n",
    "        \n",
    "    return True#, np.std(cost)    #average over 10 runs     \n",
    "\n",
    "\n",
    "\n",
    "df,OriCenters = load_dataset(dataset,10)\n",
    "dfValues = df.values\n",
    "CostVanillaKmeans1(dfValues)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604ca809",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddvfebe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce1fedf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Algo():\n",
    "    return []\n",
    "\n",
    "def LowerBoundCapSOTA(Ktarget):\n",
    "    \n",
    "    seeds=[0,100,200,300,400,500,600,700,800,900]\n",
    "    for seed in seeds:\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        w_list = Algo()\n",
    "        \n",
    "    return w_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e648d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utility functions\n",
    "\n",
    "def CostVanillaKmeanspp(Ktarget,dfValues):\n",
    "    #k-means++\n",
    "\n",
    "    seeds=[0,100,200,300,400,500,600,700,800,900]\n",
    "    cost = []\n",
    "    for seed in seeds:\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        kmean_obj = KMeans(n_clusters=Ktarget, init='k-means++' ,random_state=seed).fit(dfValues)\n",
    "        \n",
    "        cost.append(kmean_obj.inertia_)    #Sum of squarred euc dis \n",
    "        \n",
    "    return np.mean(cost), np.std(cost)     #average over 10 runs  \n",
    " \n",
    "    \n",
    "def CostVanillaKmeansList(Ktarget,dfValues):\n",
    "    \n",
    "    \n",
    "    seeds=[0,100,200,300,400,500,600,700,800,900]\n",
    "    cost = []\n",
    "    for seed in seeds:\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        kmean_obj = KMeans(n_clusters=Ktarget, init='random' ,random_state=seed).fit(dfValues)\n",
    "        \n",
    "        cost.append(kmean_obj.inertia_)    #Sum of squarred euc dis \n",
    "        \n",
    "        labels = kmean_obj.labels_ \n",
    "        \n",
    "        cluster_centers = kmean_obj.cluster_centers_\n",
    "        \n",
    "        cost_seed = []\n",
    "        for i in range(Ktarget):\n",
    "            cluster_data = dfValues[labels == i]\n",
    "            distances = distance.cdist(cluster_data, [cluster_centers[i]], 'euclidean')\n",
    "            cost_seed.append( np.sum(distances ** 2))\n",
    "        cost.append(np.max(cost_seed))\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    return cost#, np.std(cost)    #average over 10 runs     \n",
    "\n",
    "def CostVanillaKmeans(Ktarget,dfValues):\n",
    "    \n",
    "    \n",
    "    seeds=[0,100,200,300,400,500,600,700,800,900]\n",
    "    cost = []\n",
    "    for seed in seeds:\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        kmean_obj = KMeans(n_clusters=Ktarget, init='random' ,random_state=seed).fit(dfValues)\n",
    "        \n",
    "        cost.append(kmean_obj.inertia_)    #Sum of squarred euc dis \n",
    "        \n",
    "    \n",
    "    \n",
    "    return np.mean(cost)#, np.std(cost)    #average over 10 runs     \n",
    "\n",
    "\n",
    "def CostVanillaKrandom(Ktarget,dfValues):\n",
    "    \n",
    "    seeds=[0,100,200,300,400,500,600,700,800,900]\n",
    "    cost = []\n",
    "    for seed in seeds:\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        n_row, n_col = dfValues.shape\n",
    "        rand_indices = np.random.choice(n_row, size = Ktarget)\n",
    "        centroids = dfValues[rand_indices]\n",
    "        \n",
    "        distances_to_centroids = distance.cdist(dfValues, centroids, 'sqeuclidean')\n",
    "        cluster_assignment = np.argmin(distances_to_centroids, axis = 1)\n",
    "        heterogeneity=0\n",
    "        for i in range(Ktarget):\n",
    "            cluster_data = dfValues[cluster_assignment == i]\n",
    "            distances = distance.cdist(cluster_data, [centroids[i]], 'euclidean')\n",
    "            heterogeneity += np.sum(distances ** 2)\n",
    "        \n",
    "        cost.append(heterogeneity)\n",
    "    \n",
    "    return np.mean(cost)#, np.std(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03ee62fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ClusteringCost(Centers,Assignments,dfValues):\n",
    "    #Centers is of form -> (indx,dfValues[indx])  , Assignment is 1d bearing centerID  dfValues are indexed rows.\n",
    "    \n",
    "    center_map = {}\n",
    "    \n",
    "    for idx, c in Centers:\n",
    "        center_map[idx]=c\n",
    "        \n",
    "    \n",
    "    cost = 0\n",
    "    idx = 0 \n",
    "    \n",
    "    for each in Assignments:\n",
    "        if each !=-1:\n",
    "            cost += EuclideanDistance(dfValues[idx], center_map[each])\n",
    "        \n",
    "        idx += 1\n",
    "        \n",
    "    return cost #sq Cost \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1bb69a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wStarHelper(Centers):\n",
    "    #Centers are of form (indx,dfValues[indx])\n",
    "    \n",
    "    distances = []\n",
    "    for idxi in range(0,len(Centers)):\n",
    "        for idx in range(0, len(Centers)):\n",
    "            if idx!=idxi:\n",
    "                dis = EuclideanDistance_(Centers[idxi][1],Centers[idx][1])\n",
    "                distances.append(dis)\n",
    "    \n",
    "    return np.min(np.array(distances))\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "166eec3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nCr(n, r):\n",
    "    # If r is greater than n, return 0\n",
    "    if r > n:\n",
    "        return 0\n",
    "     \n",
    "    # If r is 0 or equal to n, return 1\n",
    "    if r == 0 or n == r:\n",
    "        return 1\n",
    "    # Initialize the logarithmic sum to 0\n",
    "    res = 0\n",
    "     \n",
    "    # Calculate the logarithmic sum of the numerator and denominator using loop\n",
    "    for i in range(r):\n",
    "        # Add the logarithm of (n-i) and subtract the logarithm of (i+1)\n",
    "        res += math.log(n-i) - math.log(i+1)\n",
    "    # Convert logarithmic sum back to a normal number\n",
    "    return round(math.exp(res))\n",
    "\n",
    "def CouponCollectorNonUniform(capacities,Ktarget):\n",
    "    \n",
    "    prob = []\n",
    "    sum_ = 0 \n",
    "    for each in capacities:\n",
    "        sum_ += each[1]\n",
    "        prob.append(each[1])\n",
    "    prob=np.array(prob)\n",
    "    \n",
    "    prob=np.round(prob/sum_,2)\n",
    "  \n",
    "    expec =0\n",
    "    lst = [i for i in range(0,Ktarget)]\n",
    "    \n",
    "    for m in range(1,Ktarget+1 ):\n",
    "        \n",
    "        multiplier= (-1)**(m-1)\n",
    "     \n",
    "        \n",
    "        \n",
    "        #pair_order_list = itertools.combinations(lst,m)  #Contains all possible tuples of size m \n",
    "        \n",
    "        #pair_order_list = [i for i in range(0,nCr(Ktarget,m))]#list(itertools.combinations(lst,m))\n",
    "    \n",
    "        sum_inner=(1.0*nCr(Ktarget,m))/(prob[0]*m)\n",
    "      \n",
    "\n",
    "        \n",
    "        #tuple of indexes (i,j,k)\n",
    "        \n",
    "        \n",
    "        \n",
    "        a = multiplier * sum_inner\n",
    "        \n",
    "        expec += (a)\n",
    "    \n",
    "    return int(np.ceil(np.round(expec,2)))\n",
    "        \n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ececa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params = {'legend.fontsize': '15',\n",
    "          'figure.figsize': (15, 5),\n",
    "         'axes.labelsize': 'xx-large',\n",
    "         'axes.titlesize':'xx-large',\n",
    "         'xtick.labelsize':'22',\n",
    "          'font.weight': '500',\n",
    "           'axes.linewidth':2,\n",
    "          'figure.titlesize':'xx-large',\n",
    "         'ytick.labelsize':'20'}\n",
    "\n",
    "plt.rcParams.update(params)\n",
    "def plotClusters(dataset,dfValues,Centers,OriCenters,runNo,algo):\n",
    "    \n",
    "    centers =[]\n",
    "    for idx, c in Centers:\n",
    "        centers.append(c)\n",
    "    centers= np.array(centers)\n",
    "    \n",
    "    plt.plot(dfValues[:,0], dfValues[:,1],'y.',label='Datapoints')\n",
    "    plt.plot(centers[:,0], centers[:,1],'r+',label='Online centers')\n",
    "    \n",
    "    \n",
    "        \n",
    "    plt.plot(OriCenters[:,0], OriCenters[:,1],'k^',markersize=5, label='Original centers')\n",
    "    \n",
    "    plt.axis('equal')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.title(dataset+': Run '+str(runNo))\n",
    "    plt.legend(ncol=3)\n",
    "    plt.savefig(algo+': '+dataset+'_'+str(len(Centers))+'_'+str(runNo)+'.png',dpi=600)\n",
    "    plt.show()\n",
    "    \n",
    "#ax1.legend(bbox_to_anchor=(-0.01,1.2),loc='upper left',ncol=3,fancybox=True, shadow=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa5d17a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ec415cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K target is  2\n",
      "Dataset loaded........\n",
      "M is  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lxc128/.local/lib/python3.6/site-packages/ipykernel_launcher.py:70: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/lxc128/.local/lib/python3.6/site-packages/ipykernel_launcher.py:112: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/lxc128/.local/lib/python3.6/site-packages/ipykernel_launcher.py:70: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/lxc128/.local/lib/python3.6/site-packages/ipykernel_launcher.py:112: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K target is  3\n",
      "Dataset loaded........\n",
      "M is  6\n",
      "K target is  5\n",
      "Dataset loaded........\n",
      "M is  12\n",
      "K target is  7\n",
      "Dataset loaded........\n",
      "M is  19\n",
      "K target is  10\n",
      "Dataset loaded........\n",
      "M is  30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lxc128/miniconda3/envs/FairClustering/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/lxc128/miniconda3/envs/FairClustering/lib/python3.6/site-packages/numpy/core/_methods.py:163: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K target is  15\n",
      "Dataset loaded........\n",
      "M is  48\n",
      "K target is  20\n",
      "Dataset loaded........\n",
      "M is  72\n",
      "K target is  25\n",
      "Dataset loaded........\n",
      "M is  96\n",
      "K target is  30\n",
      "Dataset loaded........\n",
      "M is  134\n",
      "K target is  40\n",
      "Dataset loaded........\n",
      "M is  214\n",
      "Opencenters_over_K_mean list  [156.4, 228.7, 345.1, 386.7, 537.6, 669.1, 794.5, 944.4, 1068.7, 1434.6]\n",
      "Opencenters_over_K_std list  [14.478950238190613, 19.48358283273382, 20.4765719787273, 14.05026690137949, 38.53362168288883, 36.45394354524624, 33.422297946131714, 34.31093120275228, 17.047287174210446, 24.175193897878046]\n",
      "Cost_over_K_mean list  [3986.1223, 3161.9242999999997, 2692.2037, 2446.3541, 1954.7415, 1650.2086000000004, 1477.4261, 1358.4876, 1292.8118, 1153.6284]\n",
      "Cost_over_K_std list  [266.45715542129847, 190.91342550907726, 153.92308623988148, 54.761057255042076, 93.20203834922275, 67.0207344561368, 34.93251649809959, 38.54757611886903, 22.555210425974785, 17.60183361016687]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KtargetList=[2,3,5,7,10,15,20,25,30,40]\n",
    "Opencenters_over_K_mean = []\n",
    "Opencenters_over_K_std=[]\n",
    "Cost_over_K_mean = []\n",
    "Cost_over_K_std=[]\n",
    "\n",
    "def COCA():\n",
    "    for Ktarget in KtargetList:\n",
    "        print('K target is ',Ktarget)\n",
    "        seeds=[0,100,200,300,400,500,600,700,800,900]\n",
    "        no_of_centers = []\n",
    "        clus_cost_list=[]\n",
    "        f_r_over_run = []\n",
    "        df,OriCenters = load_dataset(dataset,Ktarget)\n",
    "        dfValues = df.values\n",
    "\n",
    "\n",
    "        #LowerBound on capacitated clustering over different seedValues \n",
    "        #w_list_sota = LowerBoundCapSOTA(Ktarget)\n",
    "        \n",
    "\n",
    "\n",
    "        capacities_optimal=[]\n",
    "\n",
    "\n",
    "        for i in range(Ktarget):\n",
    "            capacities_optimal.append((i,np.ceil(len(dfValues)/Ktarget)))  #(centerId,capacity)\n",
    "\n",
    "\n",
    "        M =  min(CouponCollectorNonUniform(capacities_optimal,Ktarget),len(dfValues))\n",
    "        N = len(dfValues)\n",
    "        Nestd= M\n",
    "        \n",
    "        \n",
    "        dfValuesM = dfValues[:M+1]\n",
    "        w_list_heurM = LowerBoundCapHeuristic(Ktarget,dfValuesM)\n",
    "        #w_list_VanillaM = CostVanillaKmeansList(Ktarget,dfValuesM)\n",
    "        #print(w_list_heurM)\n",
    "        #print(w_list_VanillaM)\n",
    "\n",
    "        print('M is ',M)\n",
    "        runNo = 0 \n",
    "        for seed in seeds:\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            \n",
    "            interval_update = []\n",
    "            f_r_over_time = []\n",
    "\n",
    "            if order=='random':\n",
    "                df=shuffleDataset(df)\n",
    "                dfValues = df.values\n",
    "\n",
    "\n",
    "            \n",
    "            centersOpened_temp = 0 # Reset to 0 after every 3k(1+log n ) centers.\n",
    "\n",
    "            Assignments = [] #Store in serial order index of center and store -1 if opened as center \n",
    "            CentersOpened = 0 \n",
    "            Centers = []\n",
    "            Capacities = []\n",
    "\n",
    "            for indx in range(0,M):\n",
    "                Centers.append((indx,dfValues[indx]))\n",
    "                Capacities.append(np.ceil(len(dfValues)/ Ktarget  ))\n",
    "                Assignments.append(-1)\n",
    "                CentersOpened+=1\n",
    "                centersOpened_temp+=1\n",
    "\n",
    "            Centers=np.array(Centers)\n",
    "\n",
    "            CenterCost =w_list_heurM[runNo]/Ktarget # Center (or facility) opening cost i.e, w*/k1\n",
    "            #print('Center opening cost is ',CenterCost)\n",
    "\n",
    "\n",
    "            #print('if > ',np.ceil(3*Ktarget*(1+np.log(indx)) ))\n",
    "            #print(centersOpened_temp)\n",
    "            #start for rest of points after first M points ie 0 to M-1 index\n",
    "            for indx in range(M,len(dfValues)):\n",
    "                \n",
    "                if indx>Nestd:\n",
    "                    Nestd=Nestd*2\n",
    "\n",
    "                #print('centers round ',centersOpened_temp)\n",
    "                if centersOpened_temp >= np.ceil(3*Ktarget*(1+np.log(Nestd)) ):  #Reset after every 3k(1+logn) centers and double the cost for next rounds\n",
    "                    CenterCost = CenterCost* 10 #It takes log(n) as log(current n read)10\n",
    "                    centersOpened_temp=0\n",
    "                    #print('*'*15)\n",
    "                    interval_update.append(indx)\n",
    "\n",
    "\n",
    "                f_r_over_time.append(np.round(CenterCost,2))\n",
    "                \n",
    "                x=dfValues[indx]\n",
    "\n",
    "                #Step 1 : Find the closest center \n",
    "                distOfCenters=sortedDistances1(Centers,x)  #returns sorted (dis,centerIndex Original) list\n",
    "\n",
    "                closest_center_index = distOfCenters[0][1]\n",
    "                closest_center_dist =  distOfCenters[0][0]\n",
    "\n",
    "                #Step 2: with p open as center and add new index and capacity and set Assignment as -1\n",
    "\n",
    "                prob_center = np.round(closest_center_dist/CenterCost,3)\n",
    "                #print('prob p is ',prob_center)\n",
    "                currProb = np.round(random.random(),3)#random.uniform(0, 1),2)   \n",
    "                #print('curr prob p is ',currProb)\n",
    "                if currProb < prob_center:\n",
    "                    #open as center \n",
    "\n",
    "\n",
    "                    Centers=np.concatenate((Centers,np.array([(CentersOpened,x)])))#Centers.append((indx,x))\n",
    "                    CentersOpened+=1\n",
    "                    centersOpened_temp+=1\n",
    "\n",
    "                    Capacities.append(np.ceil(len(dfValues)/ Ktarget  ))\n",
    "                    Assignments.append(-1)\n",
    "                else:\n",
    "                    #Step 3 else with 1-p check if that index of cluster is not full , put Assignment as inde\n",
    "\n",
    "\n",
    "                    if Capacities[closest_center_index] >0  :\n",
    "\n",
    "                        Capacities[closest_center_index]  -=1\n",
    "                        Assignments.append(closest_center_index)\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        #Step 4: else else open as center and add new index and capacity and set -1 Assignment\n",
    "\n",
    "\n",
    "                        Centers=np.concatenate((Centers,np.array([(CentersOpened,x)])))#Centers.append((indx,x))\n",
    "                        CentersOpened+=1\n",
    "                        centersOpened_temp+=1\n",
    "                        Capacities.append(np.ceil(len(dfValues)/ Ktarget  ))\n",
    "                        Assignments.append(-1)\n",
    "\n",
    "            #print('Total online centers are ',CentersOpened)\n",
    "            no_of_centers.append(CentersOpened)\n",
    "            clus_cost = np.round(ClusteringCost(Centers,Assignments,dfValues),3)\n",
    "            clus_cost_list.append(clus_cost)\n",
    "            algo='COCA'\n",
    "            if dataset=='Synthetic-2d' or dataset=='Synthetic-2d-over':\n",
    "                plotClusters(dataset,dfValues,Centers,OriCenters,runNo,algo)\n",
    "            runNo+=1\n",
    "        Opencenters_over_K_mean.append(np.mean(no_of_centers))\n",
    "        Opencenters_over_K_std.append(np.std(no_of_centers))\n",
    "        Cost_over_K_mean.append(np.mean(clus_cost_list))\n",
    "        Cost_over_K_std.append(np.std(clus_cost_list))\n",
    "        \n",
    "        f_r_over_run.append(f_r_over_time)\n",
    "        ##print('interval update list ',list(interval_update))\n",
    "        \n",
    "        f_r_over_run=np.mean(f_r_over_run,axis=0) \n",
    "        ##print('f_r_over_run list ',list(f_r_over_run))\n",
    "    \n",
    "    print('Opencenters_over_K_mean list ', Opencenters_over_K_mean)\n",
    "    print('Opencenters_over_K_std list ', Opencenters_over_K_std)\n",
    "    print('Cost_over_K_mean list ', Cost_over_K_mean)\n",
    "    print('Cost_over_K_std list ', Cost_over_K_std)\n",
    "\n",
    "\n",
    "\n",
    "    return True#no_of_centers,clus_cost_list\n",
    "    \n",
    "COCA()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
